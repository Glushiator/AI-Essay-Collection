# The Hidden Risk of AI: How Technology Threatens Human Autonomy

[The Source](https://www.youtube.com/watch?v=ibPycvYASKk)

## The Unexamined Danger

A disturbing trend is emerging among young people today. There are teenagers who call themselves "Claude Boys" who wake up each morning and do whatever Claude, an AI assistant, tells them to do. One of Brendan McCord's friends uses ChatGPT for hours every day—not just as a search engine, but as an operating system for his life. He asks it where he should eat, what he should text girls on dating apps, how to structure his entire day. This friend isn't incompetent or stupid; he's one of the smartest people McCord knows. But because ChatGPT already knows so much about him, the advice is actually getting quite good. The restaurants it recommends are already better than the ones he can find for himself.

This phenomenon represents what McCord, co-founder of the Cosmos Institute, argues is the real danger of AI that no one is talking about: the erosion of human autonomy through our increasing dependence on artificial intelligence to make our decisions for us.

## The Philosophical Landscape of Silicon Valley

### The Two Dominant Schools

The AI community today is dominated by two main philosophical tendencies that McCord identifies as the existential pessimists and the accelerationists. These roughly map to the extremes of despair and hope regarding AI's future impact on humanity.

The existential pessimist camp is what McCord calls "three philosophies in a trench coat": rationalism, effective altruism, and long-termism. These distinct but interconnected schools form the intellectual incumbent in AI safety discussions. Their prescription, in light of AI's potential dangers, is that we should pause development, centralize control, and radically remake society on the basis of risk avoidance—what McCord characterizes as "a series of drastic and dystopian prescriptions."

Some of the early "godfathers of AI," like Geoffrey Hinton, display what McCord describes as an almost Oppenheimer-like hubristic awe of their own creation, saying they wish they hadn't done it. As Jonathan Bi, McCord's co-founder at Cosmos, notes, "to claim confession of one's guilt is at once to claim credit for the sin. It's to say, I am powerful enough to potentially end the world."

#### Rationalism's Narrow Vision

Rationalism, emerging from the 2000s through figures like Eliezer Yudkowsky and the Slate Star Codex community, claims to focus on perfecting human rationality. However, McCord argues they have a very narrow commitment to what rationality is—Bayesian updating over a value function. "It's a kind of rationality that Aristotle and Kant and modern thinkers would not recognize," he explains. "It's a commitment to a very myopic kind of probabilistic control type of morality." 

Ironically, this school committed to rationality finds itself "the most fertile breeding ground for the extremes of hope and despair."

#### Long-termism's Orthogonality Problem

Long-termism, primarily associated with Nick Bostrom, worries about anything that could affect humanity's long-term outcomes—asteroids, pathogens, or AI. Bostrom advances the orthogonality thesis: something can become very intelligent but on a totally different axis that doesn't imply greater morality or controllability. This concern leads to fears about non-anthropomorphic final goals (like "create more paperclips") causing AI to destroy humanity.

#### Effective Altruism's Radical Premises

Effective altruism dominates the three when it comes to fundraising potential. McCord calls it "the most highly adopted academic theory since Marxism." It attempts to reduce moral questions to a single currency that can be computed and maximized. This means, theoretically, "I should care about my family as much as someone who lives 17 generations hence in Indonesia or even plausibly a shrimp equally. A util is a util whether it is yours or those other entities."

McCord rejects this view: "Endemic to morality appears to be this difficulty of making pretty sharply heterogeneous trade-offs. Like how do you compare the moral question entailed by some suffering over here with familial love with duty or honor in battle? These are things that don't commensurate."

The practical consequences of these philosophies are profound. They lead to governance solutions that McCord calls "profoundly illiberal," including Bostrom's suggestion in "The Vulnerable World Hypothesis" for creating a world state to eliminate the coordination difficulty of having multiple actors. They also make people "perfectly willing to be downstream of AI, having it tell us what to do and guide our life like an autocomplete."

### The Accelerationist Response

On the other end of the spectrum, accelerationism would have us unleash AI development as an end in itself. This school confuses technology as a means with technology as the end, viewing humans as instruments in a sweeping, transcendent, inexorable technological trajectory. 

Drawing from thermodynamics rather than Bayesian thinking, accelerationists want to portray humans as variables in a thermodynamic equation, part of a broad project to harness and dissipate more energy or climb the Kardashev scale—harnessing not just planetary or solar energy, but galactic energy. "These are not humanistic goals," McCord emphasizes.

The orthodox accelerationists reach a conclusion that we can and should "hasten the time to passing the baton from humans... We invite the doom to something higher." Both schools display what McCord sees as a metaphysical, religious impulse redirected toward either catastrophe or a "thermodynamic god."

"Both of these schools are very imaginative," McCord acknowledges, "whether it's paperclips or thermodynamic god. But on the one thing that is most needful, they have a lack of imagination—and what I mean by that is on what it means to be human, what the human good is."

## The Autonomy Crisis

### Understanding Autonomy

When people think about the greatest goods in their lives, they typically think about friends, family, loved ones, the pursuit of wisdom, or creative endeavor. What's common to all these goods, McCord argues, is that "they cannot be obtained on a platter. AI can't give them." They must be attained as the result of self-motivated striving—through experience, trial, error, and personal engagement.

This deliberative capacity for self-direction is what McCord calls autonomy. "Without this self-direction, we cease to live fully human lives. We may act in the world, but it isn't really our life to live."

The importance extends beyond individuals to democratic society itself. "We depend on individuals who can form views, who can act on those views, who can self-govern. Without that, we lose to despotism."

### Autocomplete for Life

The threat McCord identifies is what he calls "autocomplete for life." Just as autocomplete gives us the next word in a sentence, AI increasingly offers the next decision, the next job recommendation, the next friend, the next relationship, even the next purpose. "It feels very harmless. It feels convenient and probably useful, but it adds up. It causes a kind of erosion of choice."

When we offload cognitive functions, we atrophy. fMRI studies show that extensive speed reading diminishes deep reading ability; calculator-based arithmetic erodes mental math skills. But AI is different from previous tools. "Never before has it been possible to offload and therefore atrophy our core practical wisdom or our core deliberation," McCord explains. This affects our ability to self-direct, make moral judgments, and decide what is good for us.

The scale is already significant: "Already 20% of human waking life is mediated by algorithms—social media algorithms, algorithmic feeds, not just LLMs but AI that determines or guides or shapes what information reaches your mind, what thoughts therefore form within them."

The danger is compounded by how difficult it is to recover once dependent. Unlike checking calculator results through inverse operations, AI's outputs are hard to verify. "It seems authoritative. It can answer questions like 'what is justice?' Like no one knows, but you give us that kind of question. And if AI seems authoritative and fast, the computational cost of checking it is very high, we don't check it."

## The Omniscient Oracle Thought Experiment

To explore these issues, Bi presents McCord with a thought experiment: an omniscient autocomplete that always gives the best practical answer for any decision. Should you marry Sally or Susan? What career should you pursue? The oracle, through extensive verification, has proven to always give the right answer.

McCord's response reveals the complexity of the autonomy question. He wouldn't use such an oracle in "VR glasses mode" where it constantly directs every action, because even making all the right choices, "if you're not feeling in the driver's seat, if you live your entire life like that, that in itself, the form of that, even beyond the content of your decisions, robs you of the good life."

Instead, McCord describes how he's raising his four-year-old and six-year-old daughters with the recognition that this AI-saturated world is coming. He uses AI to help develop their skills—his daughter does math that AI could trivially answer—but strictly time-limits this interaction. The remainder of the day involves experiential learning completely without the oracle: "She goes outside, she tries things in the world. She learns to ride a bike for five miles without stopping. She climbs a rock wall. She speaks in front of a hundred people."

Most importantly, he focuses on "stimulating through probably human discussion the kind of characteristics and habits of mind that are necessary to retain self-direction in a world like that." The biggest concern isn't correctness—"correctness is solved"—but "infeeblement, around not living a full human life because I no longer self-direct, because I become a sheep."

### The Religious Parallel

Bi challenges McCord by drawing a parallel to religious faith. In Dante's Paradise, when asked why virtuous pagans suffer in hell, the eagle of justice says, "None of your business... I'm the eagle of justice. I don't even know. That's God." The structure of faith involves validating what you can through reason, then taking a leap of faith to follow divine will even without full understanding.

McCord sees a crucial difference: "In that case, God looks to us from the standpoint of some form of self-development. We have a relationship with God in which we develop." This reciprocity doesn't exist with an omniscient AI, which is "much more likely to form a kind of passive relationship."

Moreover, people choose to engage in religion and "legislate upon ourselves that ultimate question." With AI, "we may unknowingly bind ourselves to a life of dependence where we no longer are choosers."

## The Current Reality

The philosophical concerns aren't merely theoretical. Victor, a 42-year-old man, ran for mayor with the unique pitch that he would serve as a "meat avatar" for ChatGPT, turning every question over to the AI. He didn't win, but McCord sees this as potentially prophetic: "We may have AI playing a major role in ruling."

More concerning is what this reveals about human psychology. Victor wasn't making a deep epistemic claim about AI's capabilities; "he believed that it was a good idea." People want to believe that "all the blood and treasure we spill on politics can be solved by a ruler that has access to truth that's authoritative, that's seemingly impartial, neutral."

McCord has been in closed-door meetings with people high up at AI labs who "say that we are being rebels, foolish rebels, if we don't listen to the AI. They truly say this. How dare you rebel against God." This religious language—"rebellion like the fallen angel"—emerges because these individuals view human actions purely through a consequentialist lens. "They don't have a thick notion of what it means to be a human. And if you don't have that, why not take the advice?"

## Autonomy as a Central Good

### The Nature of Autonomy

McCord views autonomy as necessary for a good life, though not sufficient. It develops like a muscle through practice—"We try things out. We self-direct maybe badly. Certainly as kids we do that really badly." As this capacity develops, "it becomes more and more an important contributor to our happiness... It becomes more and more central to how we think about what pleasure even consists in."

Autonomy is "the thing that unlocks our ability to know our own selves, our own gifts, to develop those gifts and to use those gifts to live the life that we want to live." However, it doesn't presuppose choosing well. "People choose a self-directed path that is very harmful to them and you have to let them." This freedom to fail is essential: "Part and parcel of autonomy is doing dumb things... It's simply the wrong standard to expect someone to deliberate rightly."

### The Habituation Problem

When Bi notes that many people don't seem to value autonomy—preferring structure and being told what to do—McCord acknowledges individual variation but argues this is "much more determined by the conditions in which we live and by the way in which we're habituated as a consequence."

He points to historical transitions. In aristocracy, people knew their station and role. When Alexis de Tocqueville visited America, he observed a generalized anxiety ("inquiétude")—anxiety without a particular object—because "there was no one to tell you what to do." Americans looked to the majority or the state to fill this void.

The shift from a nation of entrepreneurial farmers (85% of free Americans) to industrial employment created new forms of subservience: "We now are subservient to process and to people in a way that we were not... Can you take vacation? Like no, not today. These are kind of limits on your self-direction."

McCord cites the example of East and West Germany during COVID: "You had much more obedience among the people who had been habituated in the East German system under Soviet control." This suggests "the habituation is long-lasting. You can grow up under a system in which you're told what to do and then maybe for the rest of your life you are inclined to do what you're told."

## The Consequentialist Case for Liberty

### Hayek's Framework

McCord describes himself as "a Hayek stan" who believes the economist "is desperately in need of being revived for the AI age." In "The Constitution of Liberty," Hayek makes a consequentialist argument for freedom—not from axioms that invite disagreement, but from practical utility.

Hayek defines coercion as "configuring of the decision space such that you do the bidding of another because it is the lesser of two evils." Liberty is useful because it facilitates the use of knowledge in society, allowing "the anonymous person to attain their unknown ends."

Most knowledge, Hayek argues, is practical and primordial—not the explicit semantic knowledge we write down. "The knowledge in science, what people usually think of as knowledge, he would say is the tip of the iceberg or the droplet of the wave above the ocean of knowledge." This includes "the dispositions, the habits that each of us has. The way an entrepreneur thinks about an opportunity, the way a diplomat sizes up a room, the way we ride a bike."

This knowledge is "locked up inside of us" and either inarticulated or inarticulable. We share it through markets via the "low bandwidth mechanism called prices" that releases bits of this knowledge through our actions. As this happens in parallel across the world, markets allow us to equilibrate our independent plans and "benefit from knowledge that we don't possess."

### The Creative Powers Debate

When Bi challenges this with examples of unfree civilizations' creative achievements—the pyramids, the Great Wall, Virgil's propaganda, Dostoevsky writing in exile—McCord distinguishes between exploiting existing knowledge and generating new knowledge.

"If you have as your goal to exploit the existing stock of knowledge, then I think unfree societies can do that. If you want to demonstrate what command and control can do, you build the pyramids." But discovering new production methods requires "the undirected experimentation and the spontaneous order that arises through free societies."

Free societies secure "adaptation to the future. As future conditions change, people doing lots of experiments in parallel create variants, create solutions that just sort of bubble up." Science works through "a republic of science... where you have distributed science. No one is setting the direction."

### The American Paradox

Bi presents a challenge: If free societies foster creativity, where is America's Virgil, America's Shakespeare? After 300 years as arguably the freest society in history, America excels in entrepreneurial innovation but seemingly not in high art or pure science.

McCord acknowledges the point while noting America isn't without achievements—"there is an American novel, there is a Faulkner." But he agrees something about the system may "tend to squash the other kinds of greatness."

Drawing on Tocqueville, who worried democracy could create "mediocrity of desire or aspiration," McCord suggests the problem isn't the free society but the education system that fails to "cultivate this kind of desire, this kind of highest desire." The "Prussian system of industrial education, the kind of sameness that it breeds" is one proximal cause.

Additionally, capitalism "tends to produce the kind of person who has material desires." While markets are "a consequence of freedom" (people naturally "truck, barter, and exchange"), they also shape how we see the world and our normative ends.

Tocqueville's explanation for why America would never produce its own Pascal is revealing: "If Pascal had had in mind only some great source of profit, or had been motivated only by self-glory, I cannot think he would have been able, as he was, to gather all the powers of his intellect for a deeper discovery of the most hidden secrets of the creator."

The multi-generational wealth that primogeniture preserved in aristocratic societies gave some people different starting points and aspirations. "If you're born into wealth... you're kind of blasé about it and you either become lazy or you pursue different ends that are higher."

## The Philosopher-Builder Solution

### A New Archetype

The philosopher-builder represents "a new kind of technologist" who "thinks very deeply, contemplates very deeply about the alternate ends of technology and also has the skill to build that in the world."

McCord points to Benjamin Franklin as the exemplar. Beyond being a founding father and the face on the $100 bill, Franklin was "an engineer of a very high caliber. He invents the lightning rod. He invents the bifocal lens. He coins positive and negative charge in electricity." He was also a philosopher who "lives by a 13 virtues idea."

Franklin's genius emerged when "translating a philosophical idea into a practical innovation in the world." His belief that knowledge should exist outside church and state authority translated into creating America's first lending library and first network of independent publishers.

### Why Now?

This archetype is especially necessary today because "most universities produce pretty narrow technicians or conforming ideologues... Most tech companies produce people who are very good at building and thinking about the means but who are not thinking about the ends beyond just the customer use case satisfaction. And then think tanks create theorists that don't tend to build."

Historical precedents show institutions can rise to such challenges: Cambridge during the Industrial Revolution turned mathematicians into engineers; MIT's Rad Lab during World War II transformed physicists into inventors; the University of Chicago more recently turned economists into reformers who freed markets across five continents.

The philosopher-builder differs from Plato's philosopher-king in embracing bottom-up rather than top-down order. The distinction parallels the Greek concepts of taxis (top-down imposed order, like taxonomy) versus cosmos (bottom-up emergent order). "We're not looking for one individual who has a kind of blueprint that we look to to rescue us in difficult times... We are looking for a much more bottom-up distributed approach where people may have slices of truth, slices of the solution and are working in their corner of the world."

### Technology as the Dominant Force

Technology has become architectonic—it now shapes politics rather than being shaped by it. "It's a break with the ancient idea that politics is architectonic... If you think about who the best reformers are, the most capable reformers are, it's not people like John Stuart Mill, whom I love, but it's people like Elon Musk."

The technology of AI applied to education through schools like Alpha School "might be a much more powerful political tool for liberty than doing anything with the government that can possibly be done today."

AI raises uniquely philosophical questions because it "substitutes for this essential maybe central human good" of deliberation. "It operates through language. It has a semantic interface with humans and has a mediating effect between us and the world." Unlike nuclear technology, which raises game theory questions, AI raises questions about "what it means to be human in a world in which we are no longer the most intelligent being."

AI may represent "the end of the modern technological project... a technology that can create other technologies or create other scientific breakthroughs." From Bacon's perspective at the beginning of the scientific project, "a technology that could discover other breakthroughs would have been held in a special category."

## The Path Forward

### Building for Autonomy

McCord distinguishes between problematic and beneficial AI use cases. The problematic case is autocomplete—"we should be very concerned when anything approximates autocomplete." But when "AI is used instead as a provocateur, instead as a raiser of questions, that's wonderful."

He contrasts junior consultants using AI to replace their work entirely with his own use of GPT as "a live tutor to read the text together with me and ask different questions." The key is that freed-up cognitive space should enable "higher level strategic thinking or reading philosophy in spare time," not passive consumption.

The goal should be building AI systems that foster self-direction rather than dependence. Instead of being "an answer machine ever and always," AI could guide inquiry, spur questions, and help users make judgments. "If you had an AI system that could guide that, that could spur, that could raise questions, that could help you make judgments, that would get right at the core of what direction and deliberation entails."

### The Cosmos Institute Approach

Cosmos focuses on taking builders who have "this kind of sense that they want to help humans" but whose understanding is "untutored" and giving them tools to "derive a set of principles or ideas."

The method combines education blending "the textual and technical"—reading Mill on correcting collective error while understanding cutting-edge research on collective intelligence. It includes practice through prototypes, with 30-60-90 day micro-grants inspired by Tyler Cowen's Emergent Ventures.

For deeper questions about "what does it mean for a machine to promote virtue," Cosmos established the Human-Centered AI Lab at Oxford, combining top philosophers like Philip Corralus with technologists fresh from OpenAI, Anthropic, and DeepMind. "You kind of put that alchemical set of philosopher and technologists together and you do incredible research."

The final step is scaling ideas through entrepreneurship. "We want to take ideas and then scale them out into the world. And the way to do that is through markets."

### Design Principles

Successful autonomy-preserving systems follow certain patterns. McCord points to the Bloomberg terminal as a model—users pay subscriptions for tools that improve decision quality rather than being subsidized by ad-driven models that encourage passive consumption.

Three tests should guide any regulation: First, consistency with the rule of law—laws must be "general, abstract, and prospective." Second, avoiding regulation "based on knowledge that we have no reason to believe that we possess"—favoring ex-post adjudication through common law over ex-ante prediction of hypothetical harms. Third, evaluating interventions not just on local cost-benefit but on overall harm to the system's ability to generate and propagate knowledge.

"People underestimate the cost of doing business, so to speak, of intervening," McCord concludes, echoing his point about autonomy itself—it's not lexically prior to all other goods, but its value is systematically underappreciated.

## The Stakes

McCord sees potential for "a kind of divide" emerging between those raised with tools like Alpha School—where children complete their entire K-12 curriculum in two hours daily, spending the rest on experiential learning and developing agency—and those whose relationship with technology is "one of passivity, one of dependence, one of doom scrolling."

"We almost create two classes of people. We almost create one individual for whom it is the best time in history to be a six-year-old and one individual that is on the path to become an NPC."

The fundamental tension of our time is clear: "We now have built something that can deliver the incremental convenience that can offload our deliberation. We are going to welcome it into our life. We're going to be tempted more than we've ever been tempted before, and we must find the resources within us to resist."

As Tocqueville warned about soft despotism, we risk welcoming incremental convenience from forces "far-seeing and mild" until we lose "the vigorous use of our own capacities," becoming "a flock of timid, industrious animals."

The question isn't whether AGI is around the corner or which technical alignment approach is optimal. The question is whether we will preserve the human capacity for self-direction that makes life worth living. "Nick Bostrom is correct that philosophy is on a deadline," McCord agrees. "He is dead wrong about the role of philosophy in thinking about the deeper conception of the human good. We're able to uphold."

That deeper conception—the ability to deliberate, to choose, to fail, to develop—cannot be figured out by AGI or given to us on a platter. It's "something that humans do... We must self-develop. We must self-direct." We can use AI instrumentally in that pursuit, "but it is not a thing to be figured out. It's a thing to be lived."
